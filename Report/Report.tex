\documentclass{article}

% Layout
\usepackage[a4paper,margin=2.5cm]{geometry}
\parindent=0pt
\frenchspacing

% Packages
\usepackage[none]{hyphenat}
\usepackage[english]{babel}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage[backend=bibtex, sorting=none]{biblatex}

% Bibliography file
\bibliography{Report.bib}

% Hyperlink configuration
\hypersetup{colorlinks,linkcolor=black,citecolor=black,filecolor=black,urlcolor=black}

% Document contents
\begin{document}

\title{Distributed sentiment analysis on GitHub commit comments}
\author{Leon Helwerda (s1034375) and Tim van der Meij (s1115731)}
\date{\today}
\maketitle

\begin{abstract}
  In this report we discuss performing sentiment analysis on GitHub commit
  comments. We describe the problems and solutions related to downloading and
  preprocessing the dataset and gaining insights into the data using a naive
  implementation that uses lists with positive and negative words. After
  manually labeling 2000 training examples, we benchmark several classification
  algorithms and we use the most promising classifier in our final
  implementation where we apply the trained classifier on the entire dataset.
  We visualize the overall sentiment per programming language to show the
  potential of our approach. Distributing work is key when working with such a
  large dataset, hence getting the data, preprocessing it and performing the
  sentiment analysis is all done in a distributed manner with help of OpenMPI
  and MapReduce.
\end{abstract}

\section{Introduction}\label{sec:introduction}
Sentiment analysis is a technique that aims to extract subjective information
from text, audio or other multimedia. This information contains, but is not
limited to, judgements or emotions on a certain topic. Sentiment analysis is
often troubled by the limitations of natural language processing and by the
large amounts of data available. Furthermore, the huge amounts of data require
that performing sentiment analysis on a single text needs to be fast, otherwise
the time required for processing all texts becomes infeasible.

In this report we focus only on sentiment analysis on textual content. We use the
GitHub commit comments dataset for our work. GitHub is an online platform that
provides source code hosting and collaboration for both open-source and
closed-source projects that use the Git version control system. When working on
source code in a version-controlled environment, it is common practice to divide the
implementation of functionality into \emph{commits}, which are small packages of
code. Once the developer is ready to have the code reviewed, then reviewers can
look at these commits and comment on them. For instance, a reviewer could add a
comment to a specific line of code and suggest an improvement. The GitHub commit
comments dataset that we use consists of exactly those comments. These comments
are particularly interesting as developers and reviewers tend to put a lot of
emotions in them, ranging from happiness when a commit is of high quality to
sadness or angriness when a commit is of low quality.

We will work in a distributed manner. Since the GitHub commit comments dataset
is very large and speed and efficiency are a concern, we aim to distribute most
of the work, including downloading and preprocessing data, over several worker
nodes. We first implement a naive algorithm to get insights into the dataset and
then apply those insights to the final implementation that uses a classifier
to quickly classify commit comments as being positive, negative or neutral. Later
on we link the commit comments to the main programming language used in the code
repository to be able to get insights into the sentiments of the developers per
language.

In Section~\ref{sec:problem}, we describe the research question in more detail 
and we give definitions of the techniques that we evaluate. The used dataset and
the implementation of the distributed sentiment analysis are discussed in 
Section~\ref{sec:dataset} and Section~\ref{sec:implementation}, respectively. 
We present our experiments in Section~\ref{sec:experiments} and discuss the
results of the experiments in Section~\ref{sec:results}. We conclude our work in
Section~\ref{sec:conclusion}.

\section{Problem statement}\label{sec:problem}
The goal of this research is to determine how we can perform sentiment analysis
on a large dataset in a distributed manner. In order to investigate this, we
develop a complete sentiment analysis framework in Python that contains all
techniques discussed in this report. The implementation is further outlined in 
Section~\ref{sec:implementation}.

% TODO: discuss sentiment analysis and goal (classification pos/neg/neutral)
% TODO: discuss distributed approach and refer to dataset section for numbers
% TODO: discuss word list approach
% TODO: discuss classifier approach
\ldots

\section{Dataset}\label{sec:dataset}
We use the GitHub commit comments dataset for our work. As described in the
introduction in Section~\ref{sec:introduction}, the GitHub commit comments
dataset contains comments that reviewers have attached to specific lines of
a Git commit or to an entire Git commit.

The dataset is provided by the GHTorrent project~\cite{ghtorrent}, which
publishes incremental and categorized dumps of data from the publicly
available GitHub API\@. We only use the commit comment dumps and the
repository dumps for our work, but, among others, issue comments, pull
request comments and events are also available. Every two months a new
incremental dump is published.

We now focus on the two types of dumps we use for our work, which are the
commit comments and repository dumps. The former is used for the sentiment
analysis, while the latter is used to assign to a commit the main programming
language of the repository that it belongs to, which is useful for one of
the experiments to be performed. Each dump consists of one BSON file, which
is a binary-encoded JSON file that is used for storing and exchanging data
in a structured manner. The BSON file is converted to JSON and preprocessed,
after which we obtain a file containing objects of the following form:

\begin{verbatim}
{
  "id": 8771097,
  "body": "It seems e.offsetX is not the way to get the relative cursor position
           in Firefox. I'm getting `undefined' in my experiments there.",
  "url": "https://api.github.com/repos/fedwiki/wiki-plugin-method/comments/8771097"
}
\end{verbatim}

We use the {\tt id} field as a unique identifier for cross-reference and
the {\tt body} field which contains the actual commit message. For the
experiments we also use the {\tt url} field to extract the name of the
repository that the commit belongs to in order to assign the main
programming language of that repository to the commit message.

The repositories dump is very similar, however, from that dump we only
extract the {\tt full\_name} field which contains the repository name and
the {\tt language} field which contains the main programming language used
in the repository.

The characteristics of both datasets (commit comments and repositories) are
listed in Table~\ref{tab:dataset}. We list details of the 2015--01--29
commit comments dump as that dump has been used for manually labeling commit
comments to create a training set, which we discuss in
Section~\ref{sec:implementation}. Furthermore we list details of the
2015--01--29 commit comments dump when all comments with non-Latin characters
(Chinese, Russian, et cetera) have been removed. Those comments are not
relevant for our research as we only focus on English commit comments. Finally
we list the details for all commit comment dumps and all repository dumps. In
the table, `preprocessed' indicates that we have applied our preprocessor to
remove unneeded fields for our research.

\begin{table}[h]
  \centering
  \begin{tabular}{|l|r|l|}
    \hline
    \textbf{Dump}                           & \textbf{\# items} & \textbf{Size}                           \\ \hline
    2015--01--29 commit comments            & 182.282           & 43~MB compressed, 267~MB uncompressed   \\ \hline
    2015--01--29 Latin-only commit comments & 165.427           & 42~MB preprocessed                      \\ \hline
    Commit comments (17 dumps)              & 1.731.251         & 434~MB compressed                       \\ \hline
    Repositories (17 dumps)                 & 12.802.797        & 10.8~GB compressed, 1.3~GB preprocessed \\
    \hline
  \end{tabular}
  \caption{Characteristics of the GitHub commit comment and repository datasets.}
  \label{tab:dataset}
\end{table}

\section{Implementation}\label{sec:implementation}
\ldots

\section{Experiments}\label{sec:experiments}
\ldots

\subsection{Results}\label{sec:results}
\ldots

\section{Conclusion}\label{sec:conclusion}
\ldots

\section{References}\label{sec:references}
% TODO: add link to GitHub and open-source repository
\printbibliography[heading=none]

\end{document}
