\documentclass{article}

% Layout
\usepackage[a4paper,margin=2.5cm]{geometry}
\parindent=0pt
\frenchspacing

% Packages
\usepackage[none]{hyphenat}
\usepackage[english]{babel}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage[backend=bibtex, sorting=none]{biblatex}
\usepackage{booktabs}

% Bibliography file
\bibliography{Report.bib}

% Hyperlink configuration
\hypersetup{colorlinks,linkcolor=black,citecolor=black,filecolor=black,urlcolor=black}

% Document contents
\begin{document}

\title{Distributed sentiment analysis on GitHub commit comments}
\author{Leon Helwerda (s1034375) and Tim van der Meij (s1115731)}
\date{\today}
\maketitle

\begin{abstract}
  In this report we discuss performing sentiment analysis on GitHub commit
  comments. We describe the problems and solutions related to downloading and
  preprocessing the dataset and gaining insights into the data using a naive
  implementation that uses lists with positive and negative words. After
  manually labeling 2000 training examples, we benchmark several classification
  algorithms and we use the most promising classifier in our final
  implementation where we apply the trained classifier on the entire dataset.
  We visualize the overall sentiment per programming language to show the
  potential of our approach. Distributing work is key when working with such a
  large dataset, hence getting the data, preprocessing it and performing the
  sentiment analysis is all done in a distributed manner with help of OpenMPI
  and MapReduce.
\end{abstract}

\section{Introduction}\label{sec:introduction}
Sentiment analysis is a technique that aims to extract subjective information
from text, audio or other multimedia. This information contains, but is not
limited to, judgements or emotions on a certain topic. Sentiment analysis is
often troubled by the limitations of natural language processing and by the
large amounts of data available. Furthermore, the huge amounts of data require
that performing sentiment analysis on a single text needs to be fast, otherwise
the time required for processing all texts becomes infeasible.

In this report we focus only on sentiment analysis on textual content. We use the
GitHub commit comments dataset for our work. GitHub is an online platform that
provides source code hosting and collaboration for both open-source and
closed-source projects that use the Git version control system. When working on
source code in a version-controlled environment, it is common practice to divide the
implementation of functionality into \emph{commits}, which are small packages of
code. Once the developer is ready to have the code reviewed, then reviewers can
look at these commits and comment on them. For instance, a reviewer could add a
comment to a specific line of code and suggest an improvement. The GitHub commit
comments dataset that we use consists of exactly those comments. These comments
are particularly interesting as developers and reviewers tend to put a lot of
emotions in them, ranging from happiness when a commit is of high quality to
sadness or angriness when a commit is of low quality.

We will work in a distributed manner. Since the GitHub commit comments dataset
is very large and speed and efficiency are a concern, we aim to distribute most
of the work, including downloading and preprocessing data, over several worker
nodes. We first implement a naive algorithm to get insights into the dataset and
then apply those insights to the final implementation that uses a classifier
to quickly classify commit comments as being positive, negative or neutral. Later
on we link the commit comments to the main programming language used in the code
repository to be able to get insights into the sentiments of the developers per
language.

In Section~\ref{sec:problem}, we describe the research question in more detail 
and we give definitions of the techniques that we evaluate. The used dataset and
the implementation of the distributed sentiment analysis are discussed in 
Section~\ref{sec:dataset} and Section~\ref{sec:implementation}, respectively. 
We present our experiments in Section~\ref{sec:experiments} and discuss the
results of the experiments in Section~\ref{sec:results}. We conclude our work in
Section~\ref{sec:conclusion}.

\section{Problem statement}\label{sec:problem}
The goal of this research is to determine how we can perform sentiment analysis
on a large dataset in a distributed manner. In order to investigate this, we
develop a complete sentiment analysis framework in Python that contains all
techniques discussed in this report. The implementation is further outlined in 
Section~\ref{sec:implementation}.

Sentiment analysis is used to extract subjective information from text, audio
or other multimedia. Examples of this subjective information are opinions,
judgements or emotions on a certain topic. Multiple types of sentiment analysis
exist, but the most used ones are classification of content into an emotion
category (such as `happy', `angry' or `disappointed') or classification of
content as being positive, negative or neutral. Since sentiment analysis
yields a probability score for all categories, we are also able to numerically
compare the sentiment in different content, making it possible to determine if
one text is more positive than another, for example. Our work focusses on
performing sentiment analysis on textual content and classifying the textual
content as being positive, negative or neutral.

Being able to automatically classify content into fixed categories can yield
previously unknown insights. Because of this, sentiment analysis is becoming
increasingly important for companies all over the world that want to know how
users review their products or services. For instance, web shops like Amazon
are interested in determining which products reviewers are positive about.
With this information Amazon might decide to recommend those products over
products with less positive reviews, thereby increasing customer satisfaction
and revenues.

Sentiment analysis is troubled by the limitations of natural language processing
as well as by the large amounts of available data. Natural language processing
limitations include, but are not limited to, making proper use of context and
detecting sarcasm. On top of that, sentiment analysis on a single text should be
fast in order to make working with a large dataset feasible.

We propose a framework that performs sentiment analysis on large datasets in a
distributed manner. We aim to distribute most operations performed by the 
framework, such as downloading and preprocessing individual data dumps and
classification. We work on the Distributed ASCI Supercomputer 3 (DAS-3) at
LIACS, which has 32 worker nodes. Distributing work over multiple worker nodes
or even over multiple processes on a single node is a key feature of the
framework as it provides a great speed-up when working with the large
datasets that we use, of which more details are given in
Section~\ref{sec:dataset}.

We implement two different approaches for performing sentiment analysis. The
first approach is the word list approach. Given a text, we split the text into
individual words and for each word we test if the word appears in fixed lists
of positive and negative words. We count the number of positive and negative
words in the text and perform a majority vote for the final classification
of positive, negative or neutral. This is a basic approach that is expected to
work reasonably well, but at the same time we forsee issues with this approach.
For instance, if the algorithm encounters the word `good', it will treat that
word as positive, but if that word is preceded by the word `not', then the
meaning is actually negative. The algorithm, because of its simplicity, will
not be able to distinguish these cases. This shows the main problem of this
approach: because we only look at individual words, we omit the context in
which these words appear.

To overcome these issues and to actually take context into account, we implement
a second approach that uses a classifier to do the classification. With this
approach we do not need any fixed word lists. The classifier will learn what
positive, negative and neutral texts are from training examples. There are two
main difficulties for this approach, namely that there is no labeled training
set available for the dataset we are working with and it is not clear which
classifier would be the best fit for the data. We therefore start by manually
labeling 2000 commit comments as positive, negative or neutral. Once we have a
training set, we can train many different classifiers, such as a random forest
classifier or a Gaussian naive Bayes classifier, and perform five-fold
cross-validation to get an idea of which classifier is the best fit for the
data. After having trained the best classifier with the training set, we treat
the data that has not been used for the training set as the test set. We
provide the test set to the classifier and let the classifier put positive,
negative or neutral labels on the commit comments based on learned information
from the training phase.

To show the potential of the latter approach, we supplement each commit
comment with the main programming language of the repository it belongs to.
We perform sentiment analysis on the test data and with the resulting labels
for each commit comment we visualize the distribution of positive and negative
commit comments per programming language. This is done in order to obtain
insight into the sentiment of developers that work with different programming
languages. It is not hard to imagine that the results of sentiment analysis on
this dataset can be used to obtain many more interesting insights.

\section{Dataset}\label{sec:dataset}
We use the GitHub commit comments dataset for our work. As described in the
introduction in Section~\ref{sec:introduction}, the GitHub commit comments
dataset contains comments that reviewers have attached to specific lines of
a Git commit or to an entire Git commit.

The dataset is provided by the GHTorrent project~\cite{ghtorrent}, which
publishes incremental and categorized dumps of data from the publicly
available GitHub API\@. We only use the commit comment dumps and the
repository dumps for our work, but, among others, issue comments, pull
request comments and events are also available. Every two months a new
incremental dump is published.

We now focus on the two types of dumps we use for our work, which are the
commit comments and repository dumps. The former is used for the sentiment
analysis, while the latter is used to assign to a commit the main programming
language of the repository that it belongs to, which is useful for one of
the experiments to be performed. Each dump consists of one BSON file, which
is a binary-encoded JSON file that is used for storing and exchanging data
in a structured manner. The BSON file is converted to JSON and preprocessed,
after which we obtain a file containing objects of the following form:

\begin{verbatim}
{
  "id": 8771097,
  "body": "It seems e.offsetX is not the way to get the relative cursor position
           in Firefox. I'm getting `undefined' in my experiments there.",
  "url": "https://api.github.com/repos/fedwiki/wiki-plugin-method/comments/8771097"
}
\end{verbatim}

We use the {\tt id} field as a unique identifier for cross-reference and
the {\tt body} field which contains the actual commit message. For the
experiments we also use the {\tt url} field to extract the name of the
repository that the commit belongs to in order to assign the main
programming language of that repository to the commit message.

The repositories dump is very similar, however, from that dump we only
extract the {\tt full\_name} field which contains the repository name and
the {\tt language} field which contains the main programming language used
in the repository.

The characteristics of both datasets (commit comments and repositories) are
listed in Table~\ref{tab:dataset}. We list details of the 2015--01--29
commit comments dump as that dump has been used for manually labeling commit
comments to create a training set, which we discuss in
Section~\ref{sec:implementation}. Furthermore we list details of the
2015--01--29 commit comments dump when all comments with non-Latin characters
(Chinese, Russian, et cetera) have been removed. Those comments are not
relevant for our research as we only focus on English commit comments. Finally
we list the details for all commit comment dumps and all repository dumps. In
the table, `preprocessed' indicates that we have applied our preprocessor to
remove unneeded fields for our research.

\begin{table}[h]
  \centering
  \begin{tabular}{l r l}
    \toprule
    \textbf{Dump}                           & \textbf{\# items} & \textbf{Size}                           \\
    \midrule
    2015--01--29 commit comments            & 182.282           & 43~MB compressed, 267~MB uncompressed   \\
    2015--01--29 Latin-only commit comments & 165.427           & 42~MB preprocessed                      \\
    Commit comments (17 dumps)              & 1.731.251         & 434~MB compressed                       \\
    Repositories (17 dumps)                 & 12.802.797        & 10.8~GB compressed, 1.3~GB preprocessed \\
    \bottomrule
  \end{tabular}
  \caption{Characteristics of the GitHub commit comment and repository datasets.}\label{tab:dataset}
\end{table}

\section{Implementation}\label{sec:implementation}
\ldots
% TODO: Group

\subsection{Preprocessor}\label{sec:preprocessor}

The preprocessor is the first step in the implementation. The dataset from 
Section~\ref{sec:dataset} consists of different types of dumps and is split 
into multiple incremental dumps that are generated bimonthly. Therefore, we 
need a tool to automatically performs the phases of downloading, extracting, 
converting, processing and filtering the dumps of GitHub data from the 
GHTorrent archives.

Firstly, we use a scraper which checks the downloads page of GHTorrent and 
creates a list of possible dumps to download. The preprocessor checks whether 
the dump file already exists, otherwise it will download the compressed dump 
and extract it. Afterwards, the extracted BSON file, a binary JSON format, is 
converted to JSON\@. At this point, we can filter fields depending on the type of 
dump.

The {\tt repos} dumps, containing the repositories dataset, are filtered on the 
`language' field and stored in an efficient format. This format called 
\emph{shelves}, is a key-value based storage which allows transparent reads and 
writes. This makes it possible to merge the shelves of each dump in order to 
obtain a full mapping from repository names to the main programming language of 
those repositories. We can use this to augment the data from the commit 
comments dataset for the use in the experiments in 
Section~\ref{sec:experiments}.

The dataset of main importance is based on the {\tt commit\_comments} dumps, 
which contain the comments that reviewers make on pieces of code and commits in 
specific repositories. These commit comments are well-suited for sentiment 
analysis due to the emotions that people express about the code. 

We filter the commit comments that have a non-ASCII Unicode character in it, in 
order to remove comments in foreign non-Latin languages, in order to focus our 
classification tools on English comments.

We extract the relevant fields, namely the ID for cross-reference, the message 
body containing the textual content itself, and a group if provided. We extract 
the repository name from the given URL in the dataset. In case of the 
`language' group, we link this repository name to the main language that we 
extracted from the {\tt repos} dumps.

The preprocessor automatically performs the chosen task with interactive 
progress output on each step. This script can therefore extract data for 
a specific group from the {\tt repos} dumps and create a shelf of the languages 
of all repositories. The {\tt commit\_comments} task filters the dumps in order 
to contain the necessary fields relevant to the commit comments, including the 
group.

Note that the preprocessor can also be run using MPI in order to parallelize 
the phases for different dumps. We can distribute the work across nodes, with 
a master that schedules new work for processes that are done with one dump, 
achieving automatic load balancing. We describe the implementation of MPI in 
our tools in Section~\ref{sec:mpi}.

\subsection{Naive analyzer}\label{sec:analyzer}

We implement a naive approach as a baseline for comparing against real 
classifiers. Given a commit comment, we split the text into separate words and 
compare each word against two word lists, respectively containing positive and 
negative words. Depending on which word list the word appears in, the score of 
the entire message is altered. If the word is not in any word list, then it 
does not affect the score. We calculate the score as an average of all the 
scores of each word ($-1$ if negative or $1$ if positive) divided by the number 
of words in one of the lists.

After data inspection, the word lists are augmented with emoticons and smileys 
that are often used on GitHub in order to express emotions and opinions 
regarding the commit. We also used a script to check which words that are not 
in either word lists occur in commit comments with a certain score, in order to 
add more words that are relevant for the scores.

During the word splitting, we sanitize the words by removing quotes and other 
non-alphabetic characters. In order to handle emoticons correctly, we use 
a specialized regular expression which considers emoticons as individual words 
instead of splitting or sanitizing them.

The {\tt id} group gives colored output, while other groups are suitable for 
use with the reducer, as explained in the relevant Section~\ref{sec:reducer}. 
The analyzer can be run using MapReduce which we explain in that section. One 
can then use the plotter for further analysis, as explained in 
Section~\ref{sec:plotter}.

\subsection{Classifier}\label{sec:classifier}

\subsection{MapReduce and reducer}\label{sec:reducer}

\subsection{Plotter}\label{sec:plotter}

\subsection{Experiment runner}\label{sec:experiment_runner}

\subsection{Labeler}\label{sec:labeler}
As a labeled training set does not yet exist for the GitHub commit comments
dataset, we have to make one ourselves. In order to streamline this process
and to collaborate on the effort, we implement a labeler. We use the commit
comments dump from 29--01--2015 to create our labeled training set.

The labeler loads this dataset, presents each commit comment individually to
the user, displays a prediction based on the naive word list approach and asks
the user whether or not he or she agrees with the prediction. If so, simply
pressing Enter will store the label for that commit comment. If not, the user
can enter `p' for positive, `n' for negative, `t' for neutral and `u' for
unknown, thereby overriding the prediction. We mark commit comments as unknown
such that the classifier ignores them if they do not contain useful content,
for instance comments containing only names of variables. We do not want to
confuse the classifier with those comments during the training phase to avoid
mispredictions.

In order to collaborate on the effort, the commit comments along with their
labels are stored in JSON format. The labeler automatically resumes where
the last collaborator left off. For example, if the first collaborated labeled
up until commit comment 500, then a second collaborator would resume labeling
at commit commit 501, thereby skipping already labeled commit comments. The
JSON file with the commit comment labels can therefore easily be put in a
version control system such that any user can work with it and continue where
others left off.

\subsection{MPI}\label{sec:mpi}

\section{Experiments}\label{sec:experiments}
\ldots

\subsection{Results}\label{sec:results}
\ldots

\section{Conclusion}\label{sec:conclusion}
\ldots

\section{References}\label{sec:references}
% TODO: add link to GitHub and open-source repository
\printbibliography[heading=none]

\end{document}
